{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing codebase embeddings with a projection matrix\n",
    "\n",
    "## Goal\n",
    "\n",
    "The outcome of this exercise is to learn a projection matrix that tailors embeddings for a codebase retrieval use case, and then measure the improvement in retrieval quality.\n",
    "\n",
    "The notebook is mostly filled out, but has a series of small gaps that you will need to fill in (everywhere you see a \"TODO\" comment):\n",
    "- Define the similarity functions (both basic and with projection matrix)\n",
    "- Define a suitable loss function\n",
    "- Construct examples for training from the pre-existing dataset\n",
    "- Complete the training loop code\n",
    "- Finish the retrieval function logic\n",
    "- Evaluate the improvement in retrieval quality\n",
    "\n",
    "## Background\n",
    "\n",
    "A basic retrieval augmented generation (RAG) system will typically use embeddings to represent a set of documents that are to be searched over. Then the user input can also be converted to an embedding, and the system will use the dot product of the two embeddings to determine the relevance of the input to the documents in the database.\n",
    "\n",
    "Many embedding models are \"symmetric\", which means that they treat user input text and documents (e.g. code snippets) in the same way. It might be preferable to calculate the embedding differently (\"asymmetrically\") for the user input because is is a fundamentally different type of text.\n",
    "\n",
    "One way of doing this is to use the same embedding model, and then apply a matrix multiplication to the embedding of the user input. What we'll try to do here is find such a matrix that can improve retrieval quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "We recommend using a virtual environment to install the necessary packages.\n",
    "\n",
    "```bash\n",
    "python3.11 -m venv env\n",
    "source env/bin/activate\n",
    "```\n",
    "\n",
    "### Install packages\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Here we generate a sample embedding with `sentence_transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 384\n",
      "Embedding: [-0.07597316801548004, -0.0052619753405451775, ..., 0.03495463356375694]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained model (this will be slow the first time)\n",
    "model = SentenceTransformer(\"all-MiniLM-L12-v2\")\n",
    "\n",
    "def embed(text):\n",
    "    embedding = model.encode([text])[0]\n",
    "    return torch.tensor(embedding, dtype=torch.float32)\n",
    "\n",
    "embedding = embed(\"Hello world\")\n",
    "dim = len(embedding)\n",
    "\n",
    "print(f\"Embedding dimension: {dim}\")\n",
    "print(f\"Embedding: [{embedding[0]}, {embedding[1]}, ..., {embedding[-1]}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity\n",
    "\n",
    "First, we'll define our definition of similarity. This can be calculated using a dot product between two embeddings. For example, if we were trying to find the similarity between a user input $x_i$ and a code snippet $x_c$, then the similarity would be\n",
    "\n",
    "$$h(x_i, x_c) = e(x_i) \\cdot e(x_c)$$\n",
    "\n",
    "Fill out the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity 1: 0.4509845972061157\n",
      "Similarity 2: 0.03275974839925766\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Define the similarity function using torch\n",
    "def similarity(x_i, x_c):\n",
    "    a = embed(x_i)\n",
    "    b = embed(x_c)\n",
    "    return a @ b\n",
    "    #return torch.exp(a) @ torch.exp(b)\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Calculate the similarity between two strings\n",
    "x_i = \"Where in the codebase do we do auth?\"\n",
    "x_c_1 = \"```python\\n# Authentication\\ndef authenticate(username, password):\\n    # Code to authenticate the user\\n```\"\n",
    "x_c_2 = \"function sum(a, b) {\\n    return a + b;\\n}\"\n",
    "\n",
    "similarity1 = similarity(x_i, x_c_1)\n",
    "similarity2 = similarity(x_i, x_c_2)\n",
    "print(f\"Similarity 1: {similarity1}\")\n",
    "print(f\"Similarity 2: {similarity2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity with projection matrix\n",
    "\n",
    "Next, we'll calculate similarity using the projection matrix\n",
    "\n",
    "$$h_\\theta(x_i, x_c) = e(x_c) \\theta e(x_i)$$\n",
    "\n",
    "Fill in the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7035, -0.0096,  0.5632,  ...,  0.0893, -1.2032,  1.9034],\n",
      "        [ 0.5609, -0.4919,  1.3014,  ...,  0.3438,  0.5613,  1.3412],\n",
      "        [ 1.1023, -1.4101, -0.6779,  ...,  0.6688,  0.3065, -0.0364],\n",
      "        ...,\n",
      "        [-1.9725, -0.3642,  1.3972,  ..., -0.6769, -0.5275, -0.9344],\n",
      "        [-0.0266,  0.1232,  0.1249,  ...,  1.1176,  0.9145, -0.6113],\n",
      "        [ 0.5806,  1.1663,  0.1401,  ...,  0.1148,  0.1335, -1.9762]])\n",
      "Similarity with projection 1: 1.1331226825714111\n",
      "Similarity with projection 2: 1.5683616399765015\n"
     ]
    }
   ],
   "source": [
    "def similarity_with_projection(x_i, x_c, P):\n",
    "    a = embed(x_i)\n",
    "    b = embed(x_c)\n",
    "    return (a @ P) @ b\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Generate a dim by dim random matrix\n",
    "P_random = torch.randn(dim, dim, dtype=torch.float32)\n",
    "print(P_random)\n",
    "\n",
    "# Calculate the similarity with the random projection matrix\n",
    "similarity_with_projection1 = similarity_with_projection(x_i, x_c_1, P_random)\n",
    "similarity_with_projection2 = similarity_with_projection(x_i, x_c_2, P_random)\n",
    "print(f\"Similarity with projection 1: {similarity_with_projection1}\")\n",
    "print(f\"Similarity with projection 2: {similarity_with_projection2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "To train and test a matrix that is more helpful than the random one above, we will use a pre-existing dataset, which includes a list of (question, relevant code snippets) pairs, which happen to have been generated by a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from XML file (dataset.xml)\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass \n",
    "class Example:\n",
    "    user_input: str\n",
    "    snippets: List[str]\n",
    "\n",
    "class DatasetParser:\n",
    "    def __init__(self, xml_file: str):\n",
    "        self.tree = ET.parse(xml_file)\n",
    "        self.root = self.tree.getroot()\n",
    "\n",
    "    def parse(self) -> List[Example]:\n",
    "        examples = []\n",
    "        \n",
    "        for example in self.root.findall('example'):\n",
    "            user_input = example.find('user_input').text\n",
    "            snippets_list = []\n",
    "            \n",
    "            for snippet in example.find('snippets').findall('snippet'):\n",
    "                # Extract code and filename from the snippet text\n",
    "                snippet_text = snippet.text.strip()\n",
    "                \n",
    "                # Parse the filename from the code block header\n",
    "                first_line = snippet_text.split('\\n')[0]\n",
    "                filename = first_line.split(' ')[1] if len(first_line.split(' ')) > 1 else None\n",
    "                \n",
    "                # Remove the code block markers and get just the code\n",
    "                code_lines = snippet_text.split('\\n')[1:-1]\n",
    "                code = '\\n'.join(code_lines)\n",
    "                \n",
    "                snippets_list.append(code)\n",
    "                \n",
    "            examples.append(Example(\n",
    "                user_input=user_input,\n",
    "                snippets=snippets_list\n",
    "            ))\n",
    "            \n",
    "        return examples\n",
    "\n",
    "\n",
    "parser = DatasetParser('dataset.xml')\n",
    "dataset = parser.parse()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct examples\n",
    "\n",
    "Convert the dataset into a set of examples that can be used to train the projection matrix. These should include both examples of input/snippet pairs where the snippet is relevant, and pairs where the snippet is not relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, you should generate a list of positive and negative pairs from the dataset\n",
    "# These will be used to train the matrix\n",
    "\n",
    "# TODO: Create example pairs from the dataset\n",
    "\n",
    "# list of tuples (user input, code snippet, 1 if snippet is relevant to user input else 0)\n",
    "example_pairs = []\n",
    "threshold = 0.7\n",
    "\n",
    "\n",
    "for example in dataset:\n",
    "    for snippet in example.snippets:\n",
    "        similarity_score = similarity(example.user_input, snippet).item()\n",
    "        if similarity_score > threshold:\n",
    "            example_pairs.append((example.user_input, snippet, 1))\n",
    "        else:\n",
    "            example_pairs.append((example.user_input, snippet, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training pairs: 16\n",
      "Number of validation pairs: 4\n"
     ]
    }
   ],
   "source": [
    "# Here we split the example pairs into training and validation sets\n",
    "np.random.shuffle(example_pairs)\n",
    "split_index = int(0.8 * len(example_pairs))\n",
    "train_pairs = example_pairs[:split_index]\n",
    "val_pairs = example_pairs[split_index:]\n",
    "\n",
    "print(f\"Number of training pairs: {len(train_pairs)}\")\n",
    "print(f\"Number of validation pairs: {len(val_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a loss function\n",
    "\n",
    "With a model to calculate similarity, and a dataset of positive and negative examples, we're almost ready to train. The last thing we need is a loss function. Design a loss function that is suitable for this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(predictions, targets):\n",
    "    return nn.BCEWithLogitsLoss()(predictions, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the projection matrix\n",
    "\n",
    "The entire training loop has been set up, except for a couple of lines to calculate the prediction given an example pair and to get $y$, which will then be used together to calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/25: validation loss: 0.6012709140777588\n",
      "Epoch 1/25: validation loss: 0.5857683420181274\n",
      "Epoch 2/25: validation loss: 0.5706733465194702\n",
      "Epoch 3/25: validation loss: 0.5560002326965332\n",
      "Epoch 4/25: validation loss: 0.5417613983154297\n",
      "Epoch 5/25: validation loss: 0.5279673337936401\n",
      "Epoch 6/25: validation loss: 0.5146280527114868\n",
      "Epoch 7/25: validation loss: 0.5017523169517517\n",
      "Epoch 8/25: validation loss: 0.4893467128276825\n",
      "Epoch 9/25: validation loss: 0.47741591930389404\n",
      "Epoch 10/25: validation loss: 0.46596160531044006\n",
      "Epoch 11/25: validation loss: 0.454982191324234\n",
      "Epoch 12/25: validation loss: 0.4444738030433655\n",
      "Epoch 13/25: validation loss: 0.4344303011894226\n",
      "Epoch 14/25: validation loss: 0.42484262585639954\n",
      "Epoch 15/25: validation loss: 0.4157010316848755\n",
      "Epoch 16/25: validation loss: 0.4069927930831909\n",
      "Epoch 17/25: validation loss: 0.39870527386665344\n",
      "Epoch 18/25: validation loss: 0.39082393050193787\n",
      "Epoch 19/25: validation loss: 0.3833339810371399\n",
      "Epoch 20/25: validation loss: 0.37621966004371643\n",
      "Epoch 21/25: validation loss: 0.3694656193256378\n",
      "Epoch 22/25: validation loss: 0.3630553185939789\n",
      "Epoch 23/25: validation loss: 0.35697269439697266\n",
      "Epoch 24/25: validation loss: 0.3512018620967865\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize the projection matrix P\n",
    "P = torch.randn(\n",
    "    dim, dim, requires_grad=True\n",
    ")\n",
    "\n",
    "# Set hyperparameters\n",
    "lr = 0.001\n",
    "num_epochs = 25\n",
    "optimizer = optim.Adam([P], lr=lr)\n",
    "epochs, types, losses, accuracies, matrices = [], [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Iterate through training pairs\n",
    "    for pair in train_pairs:\n",
    "        # TODO: Get `prediction` and `y` to pass to `loss_func` \n",
    "        x_i, x_c, y = pair\n",
    "        prediction = similarity_with_projection(x_i, x_c, P)\n",
    "        y = torch.tensor([y], dtype=torch.float32)\n",
    "\n",
    "        loss = loss_func(prediction.unsqueeze(0), y)\n",
    "        loss.backward()\n",
    "    \n",
    "    # Update weights using Adam optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate validation loss\n",
    "    val_loss = 0\n",
    "    for pair in val_pairs:\n",
    "        # TODO: Get `prediction` and `y` to pass to `loss_func`\n",
    "        x_i, x_c, y = pair\n",
    "        prediction = similarity_with_projection(x_i, x_c, P)\n",
    "        y = torch.tensor([y], dtype=torch.float32)\n",
    "        \n",
    "        val_loss += loss_func(prediction.unsqueeze(0), y)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs}: validation loss: {val_loss.item() / len(val_pairs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval strategy\n",
    "\n",
    "We now have a potentially improved embedding model, but need to use it for retrieval. Finish the retrieval function, which will take a user input and return relevant code snippets from the full list. Note: a vector database is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_snippets = []\n",
    "\n",
    "for example in dataset:\n",
    "    for snippet in example.snippets:\n",
    "        all_snippets.append(snippet)\n",
    "\n",
    "# Use similarity search with the embeddings model to retrieve relevant snippets\n",
    "def retrieve_relevant_snippets(user_input: str):\n",
    "    user_embedding = embed(user_input)\n",
    "    projected_user_embedding = user_embedding @ P\n",
    "\n",
    "    similarities = []\n",
    "\n",
    "    for snippet in all_snippets:\n",
    "        snippet_embedding = embed(snippet)\n",
    "        similarity = (projected_user_embedding @ snippet_embedding).item()\n",
    "        similarities.append((snippet, similarity))\n",
    "    return similarities\n",
    "    raise NotImplementedError.add_note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the new retrieval strategy\n",
    "\n",
    "If the loss was lower by the last epoch, then we know that we improved the similarity function (at least for the validation set), but we still need a way of evaluating the retrieval strategy as a whole.\n",
    "\n",
    "Your last task is to design an evaluation metric suitable for codebase retrieval, which we can run over the examples in the above dataset. The result of the evaluation should be a single number that attempts to represent the quality of the retrieval strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('def initiate_password_reset(email):\\n    token = generate_reset_token()\\n    send_reset_email(email, token)\\n    store_reset_token(email, token, expiry=24*hours)\\n    return True\\n\\ndef validate_reset_token(token, new_password):\\n    if is_token_valid(token):\\n        user = get_user_by_token(token)\\n        update_password(user, new_password)\\n        invalidate_token(token)\\n        return True\\n    return False',\n",
       "  -1.4833674430847168),\n",
       " ('export class UserService {\\n  async register(userData: RegisterDTO): Promise<User> {\\n    const existingUser = await this.userRepo.findByEmail(userData.email);\\n    if (existingUser) {\\n      throw new DuplicateUserError();\\n    }\\n    \\n    const hashedPassword = await bcrypt.hash(userData.password);\\n    return this.userRepo.create({\\n      ...userData,\\n      password: hashedPassword\\n    });\\n  }\\n}',\n",
       "  -0.5153960585594177),\n",
       " (\"class PaymentProcessor {\\n  async processPayment(amount: number, paymentMethod: PaymentMethod): Promise<PaymentResult> {\\n    const transaction = await this.stripeClient.createCharge({\\n      amount,\\n      currency: 'usd',\\n      payment_method: paymentMethod.id,\\n      confirm: true\\n    });\\n    \\n    return this.saveTransaction(transaction);\\n  }\\n}\",\n",
       "  -1.0435307025909424),\n",
       " (\"from sendgrid import SendGridAPIClient\\n\\nclass EmailService:\\n    def __init__(self, api_key):\\n        self.client = SendGridAPIClient(api_key)\\n    \\n    def send_email(self, to_email, subject, content):\\n        message = {\\n            'to': to_email,\\n            'subject': subject,\\n            'content': content,\\n            'from_email': 'noreply@ourapp.com'\\n        }\\n        return self.client.send(message)\",\n",
       "  -1.265540599822998),\n",
       " (\"@Controller('uploads')\\nexport class FileUploadController {\\n  @Post()\\n  async uploadFile(@UploadedFile() file: Express.Multer.File) {\\n    const result = await this.s3Service.upload(file.buffer, {\\n      bucket: 'user-uploads',\\n      contentType: file.mimetype\\n    });\\n    \\n    return { url: result.Location };\\n  }\\n}\",\n",
       "  -0.13141822814941406),\n",
       " (\"module.exports = {\\n  development: {\\n    client: 'postgresql',\\n    connection: {\\n      host: process.env.DB_HOST,\\n      database: process.env.DB_NAME,\\n      user: process.env.DB_USER,\\n      password: process.env.DB_PASSWORD\\n    },\\n    pool: {\\n      min: 2,\\n      max: 10\\n    }\\n  }\\n}\",\n",
       "  1.0759981870651245),\n",
       " (\"export class RateLimiter {\\n  constructor(private redis: Redis) {}\\n\\n  async limit(req: Request, res: Response, next: NextFunction) {\\n    const key = `rate_limit:${req.ip}`;\\n    const requests = await this.redis.incr(key);\\n    \\n    if (requests === 1) {\\n      await this.redis.expire(key, 60);\\n    }\\n    \\n    if (requests > 100) {\\n      return res.status(429).json({ error: 'Too many requests' });\\n    }\\n    \\n    next();\\n  }\\n}\",\n",
       "  -1.8475438356399536),\n",
       " (\"export class SearchService {\\n  async search(query: string, filters: SearchFilters): Promise<SearchResult[]> {\\n    const elasticQuery = this.buildElasticQuery(query, filters);\\n    \\n    const results = await this.elasticClient.search({\\n      index: 'products',\\n      body: elasticQuery\\n    });\\n    \\n    return this.mapResults(results.hits);\\n  }\\n}\",\n",
       "  -1.2190184593200684),\n",
       " (\"import winston from 'winston';\\n\\nexport const logger = winston.createLogger({\\n  level: process.env.LOG_LEVEL || 'info',\\n  format: winston.format.json(),\\n  transports: [\\n    new winston.transports.File({ filename: 'error.log', level: 'error' }),\\n    new winston.transports.File({ filename: 'combined.log' })\\n  ]\\n});\",\n",
       "  1.0589131116867065),\n",
       " (\"export class CacheService {\\n  constructor(private redis: Redis) {}\\n  \\n  async get<T>(key: string): Promise<T | null> {\\n    const cached = await this.redis.get(key);\\n    return cached ? JSON.parse(cached) : null;\\n  }\\n  \\n  async set(key: string, value: any, ttl?: number): Promise<void> {\\n    await this.redis.set(key, JSON.stringify(value), 'EX', ttl || 3600);\\n  }\\n}\",\n",
       "  -2.1345252990722656),\n",
       " (\"@WebSocketGateway()\\nexport class WebSocketGateway {\\n  @WebSocketServer()\\n  server: Server;\\n\\n  handleConnection(client: Socket) {\\n    const userId = this.getUserFromToken(client.handshake.auth.token);\\n    client.join(`user:${userId}`);\\n  }\\n\\n  @SubscribeMessage('message')\\n  handleMessage(client: Socket, payload: any) {\\n    this.server.to(payload.room).emit('message', payload);\\n  }\\n}\",\n",
       "  -0.5450282096862793),\n",
       " ('export class ValidationMiddleware {\\n  validate(schema: Joi.Schema) {\\n    return (req: Request, res: Response, next: NextFunction) => {\\n      const { error } = schema.validate(req.body);\\n      \\n      if (error) {\\n        return res.status(400).json({\\n          error: error.details[0].message\\n        });\\n      }\\n      \\n      next();\\n    };\\n  }\\n}',\n",
       "  -1.5316389799118042),\n",
       " ('export class CartService {\\n  async addToCart(userId: string, productId: string, quantity: number) {\\n    const cart = await this.cartRepo.findByUser(userId);\\n    const product = await this.productRepo.findById(productId);\\n    \\n    if (product.stock < quantity) {\\n      throw new InsufficientStockError();\\n    }\\n    \\n    return this.cartRepo.addItem(cart.id, {\\n      productId,\\n      quantity,\\n      price: product.price\\n    });\\n  }\\n}',\n",
       "  0.18905460834503174),\n",
       " ('export class NotificationService {\\n  async notify(userId: string, notification: Notification) {\\n    const user = await this.userRepo.findById(userId);\\n    \\n    if (user.preferences.email) {\\n      await this.emailService.send(user.email, notification);\\n    }\\n    \\n    if (user.preferences.push) {\\n      await this.pushService.send(user.deviceToken, notification);\\n    }\\n  }\\n}',\n",
       "  -0.8587836623191833),\n",
       " (\"export class ErrorHandler {\\n  catch(error: Error, req: Request, res: Response, next: NextFunction) {\\n    if (error instanceof ValidationError) {\\n      return res.status(400).json({\\n        type: 'ValidationError',\\n        message: error.message\\n      });\\n    }\\n    \\n    if (error instanceof AuthenticationError) {\\n      return res.status(401).json({\\n        type: 'AuthenticationError',\\n        message: 'Unauthorized'\\n      });\\n    }\\n    \\n    // Default error\\n    res.status(500).json({\\n      type: 'ServerError',\\n      message: 'Internal server error'\\n    });\\n  }\\n}\",\n",
       "  -0.2967683970928192),\n",
       " ('export class ProfileService {\\n  async updateProfile(userId: string, updates: ProfileUpdates): Promise<User> {\\n    const user = await this.userRepo.findById(userId);\\n    \\n    if (updates.email && updates.email !== user.email) {\\n      await this.verifyEmailUnique(updates.email);\\n    }\\n    \\n    const updatedUser = await this.userRepo.update(userId, updates);\\n    return this.sanitizeUser(updatedUser);\\n  }\\n}',\n",
       "  -0.7633914947509766),\n",
       " (\"export class PDFGenerator {\\n  async generatePDF(template: string, data: any): Promise<Buffer> {\\n    const html = await this.templateEngine.render(template, data);\\n    \\n    const browser = await puppeteer.launch();\\n    const page = await browser.newPage();\\n    await page.setContent(html);\\n    \\n    const pdf = await page.pdf({\\n      format: 'A4',\\n      printBackground: true\\n    });\\n    \\n    await browser.close();\\n    return pdf;\\n  }\\n}\",\n",
       "  0.10444784164428711),\n",
       " (\"export class JobProcessor {\\n  @Process('email')\\n  async processEmailJob(job: Job) {\\n    const { to, subject, content } = job.data;\\n    await this.emailService.send(to, subject, content);\\n  }\\n\\n  @Process('image-resize')\\n  async processImageResize(job: Job) {\\n    const { imageUrl, dimensions } = job.data;\\n    const resized = await this.imageService.resize(imageUrl, dimensions);\\n    await this.storageService.upload(resized);\\n  }\\n}\",\n",
       "  -0.5048574209213257),\n",
       " ('class RecommendationEngine:\\n    def generate_recommendations(self, user_id):\\n        user_history = self.get_user_history(user_id)\\n        similar_users = self.find_similar_users(user_history)\\n        \\n        recommendations = []\\n        for user in similar_users:\\n            items = self.get_user_items(user)\\n            recommendations.extend(self.filter_unseen_items(items, user_history))\\n        \\n        return self.rank_recommendations(recommendations)',\n",
       "  0.23469966650009155),\n",
       " ('export class AnalyticsService {\\n  async trackEvent(event: AnalyticsEvent) {\\n    await this.segmentClient.track({\\n      userId: event.userId,\\n      event: event.name,\\n      properties: event.properties,\\n      timestamp: new Date()\\n    });\\n    \\n    await this.store.incrementEventCount(event.name);\\n  }\\n}',\n",
       "  1.0652059316635132)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_relevant_snippets(\"Reset password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP): 0.1799\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def average_precision(relevant_items: List[str], retrieved_items: List[Tuple[str, float]]) -> float:\n",
    "    relevant_set = set(relevant_items)\n",
    "    precision_sum = 0\n",
    "    relevant_count = 0\n",
    "\n",
    "    for i, (item, _) in enumerate(retrieved_items, 1):\n",
    "        if item in relevant_set:\n",
    "            relevant_count += 1\n",
    "            precision_sum += relevant_count / i\n",
    "\n",
    "    return precision_sum / len(relevant_items) if relevant_items else 0\n",
    "\n",
    "def evaluate_retrieval_strategy(retrieval_strategy):\n",
    "    total_ap = 0\n",
    "    num_queries = 0\n",
    "\n",
    "    for example in dataset:\n",
    "        retrieved = retrieval_strategy(example.user_input)\n",
    "        ap = average_precision(example.snippets, retrieved)\n",
    "        total_ap += ap\n",
    "        num_queries += 1\n",
    "\n",
    "    map_score = total_ap / num_queries if num_queries > 0 else 0\n",
    "    return f\"Mean Average Precision (MAP): {map_score:.4f}\"\n",
    "\n",
    "result = evaluate_retrieval_strategy(retrieve_relevant_snippets)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
